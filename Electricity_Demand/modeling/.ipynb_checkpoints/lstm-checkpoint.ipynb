{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis is **heavily** adapted from [this tutorial](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/) on how to apply LSTMs in Python and [this blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) describing how RNNs and LSTMs work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will a type of recurrent neural networks (RNNs) known as long short term memory (LSTM) to predict electricity demand from weather features. Whereas traditional NNs have no baseline sense of temporality, RNNs include loops, allowing information to persist across multiple timesteps. **Figure 1** shows an unraveled RNN and demonstrates how information persists through time; by supplying the output of a previous timestep to the input of the current timestep, past information is used to predict the future.\n",
    "\n",
    "![\"An unrolled RNN\"](RNN-unrolled.png \"Unrolled RNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1: Unraveled RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with RNNs are long-term dependencies: it's difficult for RNNs to recall pertinent information from timesteps far away from the current timestep. Resultingly, LSTM networks were desdigned by [Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) to allow for long-term dependencies. **Figure 2** shows the internal structure of an LSTM, each part of which will be discussed below.\n",
    "![\"LSTM\"](LSTM-chain.png \"LSTM diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2: LSTM diagram. The pink icons represent matrix transformations (addition, multiplication, tanh) while the yellow icons represent different network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind LSTMs is the cell state: the horizontal line running across the top of the LSTM cell. Crucially, the LSTM has the ability to add or remove information from the cell state through structures called gates.\n",
    "\n",
    "Data flows through the layers of an LSTM in a sequence of steps:\n",
    "1. First the forget gate layer, which looks at the previous LSTMs output and the current input and decides how much information should be removed from the cell state between timesteps.\n",
    "2. Second the input gate layer, which decides which values to update and importantly *how much* to update each value in the cell state.\n",
    "3. Third the tanh layer, which creates a vector of candidate information to be possibly added to the cell state.\n",
    "4. Fourth we combine the previous steps by first removing the information we decided to forget in step one and then adding the candidate values decided in step three scaled by how much we decided to update each state value in step two. The product here is the new cell state, which will be transfered to the subsequent LSTM.\n",
    "5. Finally we decide what to output ($h_t$ in the diagram). The output will be based on our cell state, but filtered according to which information we decided to add and remove before.\n",
    "\n",
    "This is a rudimentary overview of RNNs and LSTMs. For a more in-depth, mathematical review, check out the blog post linked at the top. Now we apply LSTMs to predict electricity demand from weather features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, col_names, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('%s(t-%d)' % (col_names[j], i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('%s(t)' % (col_names[j])) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('%s(t+%d)' % (col_names[j], i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORKING_DIR = '/Users/rvg/Documents/springboard_ds/springboard_portfolio/Electricity_Demand/'\n",
    "\n",
    "la_df = pd.read_pickle(WORKING_DIR + 'data/LA_df.pkl')\n",
    "\n",
    "seattle_df = pd.read_pickle(WORKING_DIR + 'data/seattle_df.pkl')\n",
    "\n",
    "#the following example will be with the LA dataset only\n",
    "dataset = la_df.copy()\n",
    "\n",
    "# set the column we want to predict (demand) to the first columns for consistency with the tutorial\n",
    "cols = list(dataset.columns)\n",
    "cols.remove('demand')\n",
    "cols.insert(0,'demand')\n",
    "dataset = dataset[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for LSTM\n",
    "\n",
    "The first step is to prepare the dataset for the LSTM.\n",
    "\n",
    "This involves framing the dataset as a supervised learning problem and normalizing the input variables.\n",
    "\n",
    "We will frame the supervised learning problem as predicting the electricity demand at the current hour (t) given the electricity demand and weather conditions at the prior time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   demand(t-1)  hourlyvisibility(t-1)  hourlydewpointtempf(t-1)  \\\n",
      "1     0.237244                      1                  0.915663   \n",
      "2     0.186420                      1                  0.927711   \n",
      "3     0.155685                      1                  0.915663   \n",
      "4     0.134592                      1                  0.891566   \n",
      "5     0.135195                      1                  0.891566   \n",
      "\n",
      "   hourlyrelativehumidity(t-1)  hourlywindspeed(t-1)  \\\n",
      "1                     0.680412                   0.0   \n",
      "2                     0.628866                   0.0   \n",
      "3                     0.515464                   0.0   \n",
      "4                     0.443299                   0.2   \n",
      "5                     0.494845                   0.2   \n",
      "\n",
      "   hourlystationpressure(t-1)  hourlyprecip(t-1)  dailyheatingdegreedays(t-1)  \\\n",
      "1                    0.466019                  0                            0   \n",
      "2                    0.466019                  0                            0   \n",
      "3                    0.456310                  0                            0   \n",
      "4                    0.417475                  0                            0   \n",
      "5                    0.417475                  0                            0   \n",
      "\n",
      "   dailycoolingdegreedays(t-1)  hourlycoolingdegrees(t-1)  \\\n",
      "1                     0.416667                   0.181818   \n",
      "2                     0.416667                   0.363636   \n",
      "3                     0.416667                   0.454545   \n",
      "4                     0.416667                   0.363636   \n",
      "5                     0.416667                   0.272727   \n",
      "\n",
      "   hourlyheatingdegrees(t-1)  hourlytimeofday(t-1)  \\\n",
      "1                          0                     0   \n",
      "2                          0                     0   \n",
      "3                          0                     0   \n",
      "4                          0                     0   \n",
      "5                          0                     0   \n",
      "\n",
      "   hourlyskyconditions_BKN(t-1)  hourlyskyconditions_CLR(t-1)  \\\n",
      "1                             0                             1   \n",
      "2                             0                             0   \n",
      "3                             0                             1   \n",
      "4                             0                             1   \n",
      "5                             0                             1   \n",
      "\n",
      "   hourlyskyconditions_FEW(t-1)  hourlyskyconditions_OVC(t-1)  \\\n",
      "1                             0                             0   \n",
      "2                             1                             0   \n",
      "3                             0                             0   \n",
      "4                             0                             0   \n",
      "5                             0                             0   \n",
      "\n",
      "   hourlyskyconditions_SCT(t-1)  demand(t)  \n",
      "1                             0   0.186420  \n",
      "2                             0   0.155685  \n",
      "3                             0   0.134592  \n",
      "4                             0   0.135195  \n",
      "5                             0   0.157091  \n"
     ]
    }
   ],
   "source": [
    "values = dataset.values\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, dataset.columns, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33]], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Fit Model\n",
    "\n",
    "In this section, we will fit an LSTM on the multivariate input data.\n",
    "\n",
    "First, we must split the prepared dataset into train and test sets. To speed up the training of the model for this demonstration, we will only fit the model on the first year of data, then evaluate it on the remaining data.\n",
    "\n",
    "We split the dataset into train and test sets, then splits the train and test sets into input and output variables. Finally, the inputs (X) are reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((8760, 1, 17), (8760,), (18294, 1, 17), (18294,))\n"
     ]
    }
   ],
   "source": [
    "values = reframed.values\n",
    "n_train_hours = 365 * 24\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define and fit our LSTM model.\n",
    "\n",
    "We will define the LSTM with 50 neurons in the first hidden layer and 1 neuron in the output layer for predicting electricity demand. The input shape will be 1 time step with 17 features.\n",
    "\n",
    "We will use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.\n",
    "\n",
    "The model will be fit for 50 training epochs with a batch size of 72. Epochs Remember that the internal state of the LSTM in Keras is reset at the end of each batch, so an internal state that is a function of a number of days may be helpful (try testing this).\n",
    "\n",
    "Finally, we keep track of both the training and test loss during training by setting the validation_data argument in the fit() function. At the end of the run both the training and test loss are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8760 samples, validate on 18294 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 0.0906 - val_loss: 0.0778\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.0611 - val_loss: 0.0683\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.0513 - val_loss: 0.0542\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.0407 - val_loss: 0.0412\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.0321 - val_loss: 0.0322\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.0279 - val_loss: 0.0266\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.0261 - val_loss: 0.0261\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.0251 - val_loss: 0.0276\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.0246 - val_loss: 0.0281\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.0243 - val_loss: 0.0280\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.0240 - val_loss: 0.0298\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.0239 - val_loss: 0.0308\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.0237 - val_loss: 0.0306\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.0236 - val_loss: 0.0308\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.0235 - val_loss: 0.0305\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.0233 - val_loss: 0.0300\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.0232 - val_loss: 0.0293\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.0230 - val_loss: 0.0282\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.0231 - val_loss: 0.0278\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.0229 - val_loss: 0.0294\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.0228 - val_loss: 0.0292\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.0228 - val_loss: 0.0279\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.0227 - val_loss: 0.0276\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.0226 - val_loss: 0.0284\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.0226 - val_loss: 0.0279\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.0226 - val_loss: 0.0273\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.0225 - val_loss: 0.0275\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.0225 - val_loss: 0.0270\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.0224 - val_loss: 0.0274\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.0223 - val_loss: 0.0275\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.0224 - val_loss: 0.0269\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.0224 - val_loss: 0.0265\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.0223 - val_loss: 0.0265\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.0223 - val_loss: 0.0265\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0223 - val_loss: 0.0262\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0222 - val_loss: 0.0265\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.0223 - val_loss: 0.0261\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0222 - val_loss: 0.0262\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0222 - val_loss: 0.0255\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0221 - val_loss: 0.0255\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.0221 - val_loss: 0.0262\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0222 - val_loss: 0.0252\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.0221 - val_loss: 0.0256\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0220 - val_loss: 0.0264\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.0221 - val_loss: 0.0256\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.0221 - val_loss: 0.0253\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.0221 - val_loss: 0.0256\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.0221 - val_loss: 0.0253\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0220 - val_loss: 0.0246\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.0220 - val_loss: 0.0256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1235c9a10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEPCAYAAABRHfM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f6a2ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MAE)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "After the model is fit, we can forecast for the entire test dataset.\n",
    "\n",
    "We combine the forecast with the test dataset and invert the scaling. We also invert scaling on the test dataset with the expected pollution numbers.\n",
    "\n",
    "With forecasts and actual values in their original scale, we can then calculate an error score for the model. In this case, we calculate the Root Mean Squared Error (RMSE) that gives error in the same units as the variable itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 164.781\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE on the test set is quite remarkable--it's almost half the RMSE of our best supervised learning algorithm! (Bagging, ~300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
